<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<h2 id="logistic-regression-in-tensorflow">Logistic regression in TensorFlow</h2>
<ol style="list-style-type: lower-alpha">
<li><p>Open the notebook <a href="https://github.com/tensorchiefs/dl_course/blob/master/notebooks/04_log_reg_challenger.ipynb">log_reg_challenger</a>. In this notebook we use (binary) logistic regression to predict the probability for an O-ring to show a damage <span class="math inline">\((y=1)\)</span> by using the temperature during take-off as predictor <span class="math inline">\(x\)</span>.<br />
<span class="math display">\[p(y_i=1 | x_i) = \frac{e^{(b + W&#39; x_i)}}{1 + e^{(b + W&#39; x_i)}} = \frac{1}{1 + e^{-(b + W&#39; x_i)}}\]</span> Determine the predicted <span class="math inline">\(p(y_i=1 | x_i)\)</span> values when using the parameters <span class="math inline">\(W=-0.2\)</span> and <span class="math inline">\(b=20\)</span>. What do you observe?</p></li>
<li><p>Now lets try to find better values for <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span>. Lets assume <span class="math inline">\(W\)</span> is given with <span class="math inline">\(-1\)</span>. We want the probability for a damage <span class="math inline">\(p(y_i=1 | x_i)\)</span> to be <span class="math inline">\(0.5\)</span>.<br />
Determine an appropriate value for <span class="math inline">\(b\)</span>.<br />
Hint: at which <span class="math inline">\(x\)</span> value should <span class="math inline">\(p(y_i=1 | x_i)\)</span> be <span class="math inline">\(0.5\)</span>, look at the data. At this <span class="math inline">\(x\)</span> value the term <span class="math inline">\(1 + e^{-(b + W&#39; x_i)}\)</span> must be <span class="math inline">\(2\)</span>.</p></li>
<li><p>Now we want to optimize the parameter values by using the gradient descent method. Run the TensorFlow forward pass in cell 5 and optimize the values for <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span> in cell 6.<br />
Fetch the loss, <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span> and print the final values.<br />
Hint: You can't use the same names for the results of your fetches as you have used for the TensorFlow graph. See cell 5.</p></li>
</ol>
<h2 id="multinomial-logistic-regression-on-mnist-dataset">Multinomial Logistic Regression on MNIST dataset</h2>
<ol style="list-style-type: lower-alpha">
<li><p>Open the notebook <a href="https://github.com/tensorchiefs/dl_course/blob/master/notebooks/05_Multinomial_Logistic_Regression.ipynb">Multinomial Logistic Regression</a>. In this notebook we use multinomial logistic regression to predict the handwritten digits of the MNIST dataset.<br />
We have 4000 examples with 784 pixel values and 10 classes. Run the fist 3 cells and explain the one-hot-encoding. In TensorFlow we need to use one-hot-encoding.</p></li>
<li><p>Write the missing TensorFlow code in cell 4 to do the required matrix multiplication between x and w and then add the bias b.<br />
<span class="math inline">\(z=x*w+b\)</span></p></li>
<li><p>Run the next two cells to store the graph and do a forward pass of the untrained network, look at the probability for each class of some examples.</p></li>
<li><p>Now lets train the model. We use a mini-batch size of 128 and use the fist 2400 examples for training.<br />
The validation set will be the examples from 2400 to 3000. Write the code to get the loss and the probabilities of your validation set. Run the last cells to check the performance of the model and to get the probability of a random example of the validation set.</p></li>
<li>Additional Questions:</li>
</ol>
<ul>
<li>How many parameters do we have?</li>
<li>Compare the loss of the validation set against the loss of the training set. Why do you think that there is such a great difference.</li>
<li>Do you understand how the loss is calculated <code>loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(prob), reduction_indices=[1]))</code></li>
</ul>
</body>
</html>
